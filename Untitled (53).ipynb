{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84b340dc-25f5-4119-b17e-c649bc47e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f10ccba3-3ad8-47fc-8bd0-9114ac509ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared, also known as the coefficient of determination, is a statistical measure used to assess the goodness-of-fit of a linear regression model. It quantifies the proportion of the variance in the dependent variable that can be explained by the independent variables included in the model. R-squared values range from 0 to 1, with a higher value indicating a better fit of the model to the data.\n",
    "\n",
    "# The calculation of R-squared involves comparing the total sum of squares (TSS) and the residual sum of squares (RSS) of the model:\n",
    "\n",
    "# Total Sum of Squares (TSS):\n",
    "# TSS measures the total variability in the dependent variable (y). It represents the sum of the squared differences between each observed y value and the mean of y.\n",
    "\n",
    "# TSS = Σ(yᵢ - ȳ)²\n",
    "\n",
    "# where yᵢ is each observed y value, and ȳ is the mean of y.\n",
    "\n",
    "# Residual Sum of Squares (RSS):\n",
    "# RSS measures the variability that remains unexplained by the regression model. It represents the sum of the squared differences between each observed y value and its corresponding predicted y value (ŷ) from the regression model.\n",
    "\n",
    "# RSS = Σ(yᵢ - ŷᵢ)²\n",
    "# where yᵢ is each observed y value, and ŷᵢ is the predicted y value from the regression model.\n",
    "\n",
    "# R-squared Calculation:\n",
    "# R-squared is calculated as the ratio of the explained sum of squares (ESS) to the total sum of squares (TSS):\n",
    "\n",
    "# R-squared = 1 - (RSS / TSS)\n",
    "\n",
    "# or\n",
    "\n",
    "# R-squared = (TSS - RSS) / TSS\n",
    "\n",
    "# R-squared represents the proportion of the variance in the dependent variable that is explained by the independent variables in the model. It provides an indication of how well the model fits the data. A higher R-squared value indicates that a larger proportion of the variation in the dependent variable is accounted for by the independent variables in the model. Conversely, a lower R-squared suggests that the independent variables do not explain much of the variation in the dependent variable.\n",
    "\n",
    "# However, it's important to note that R-squared should not be the sole criterion for evaluating a model. It does not indicate the correctness of the regression model or the significance of the independent variables. It only assesses the goodness-of-fit and the proportion of explained variance. Therefore, it is recommended to consider other metrics, such as adjusted R-squared, p-values, and residual analysis, to obtain a comprehensive evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cad958e-a668-4465-8b2c-d5353ea6fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ded7a41-cf31-45b1-8cb6-2f05711bf439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted R-squared is a modified version of the R-squared statistic that takes into account the number of predictors (independent variables) in a linear regression model. While regular R-squared provides a measure of the proportion of variance explained by the predictors, adjusted R-squared adjusts the R-squared value to penalize the inclusion of unnecessary or irrelevant predictors.\n",
    "\n",
    "# The formula to calculate adjusted R-squared is:\n",
    "\n",
    "# Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "# where:\n",
    "\n",
    "# R-squared is the regular coefficient of determination.\n",
    "# n is the number of observations in the dataset.\n",
    "# k is the number of predictors (independent variables) in the model.\n",
    "# The difference between adjusted R-squared and regular R-squared lies in the penalty factor applied to the number of predictors. Adjusted R-squared adjusts for the degrees of freedom associated with the number of predictors included in the model. It accounts for the potential bias introduced when adding more predictors, especially if they do not contribute significantly to explaining the dependent variable.\n",
    "\n",
    "# Here are the key differences between adjusted R-squared and regular R-squared:\n",
    "\n",
    "# 1. Penalizes Model Complexity: Adjusted R-squared penalizes the addition of unnecessary predictors to the model by adjusting for the number of predictors. As the number of predictors increases, the adjusted R-squared value will decrease if those predictors do not contribute significantly to explaining the dependent variable. Regular R-squared does not account for the complexity introduced by additional predictors.\n",
    "\n",
    "# 2. Encourages Model Parsimony: Adjusted R-squared encourages model simplicity and parsimony by giving more weight to models with fewer predictors that still explain a significant amount of variance. It provides a way to compare models with different numbers of predictors, allowing for a fair comparison of their performance.\n",
    "\n",
    "# 3. Lower Values: In general, adjusted R-squared values will be lower than regular R-squared values for the same model. This is because adjusted R-squared takes into account the number of predictors and applies a penalty for model complexity.\n",
    "\n",
    "# 4. Model Selection: Adjusted R-squared can be used as a criterion for model selection. Comparing adjusted R-squared values across different models with varying numbers of predictors can help identify the model that strikes a balance between goodness-of-fit and model complexity.\n",
    "\n",
    "# Adjusted R-squared is a useful metric to evaluate the quality and parsimony of a regression model, especially when comparing models with different numbers of predictors. It provides a more robust measure of model performance by accounting for the potential overfitting that can occur when adding unnecessary predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "753c6f46-02c7-4d18-992c-8d4c453ec401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa49e314-d3b4-493b-8bac-9604445732e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted R-squared is more appropriate to use in the following situations:\n",
    "\n",
    "# 1. Model Comparison: When comparing multiple regression models with different numbers of predictors, adjusted R-squared provides a fair basis for model comparison. It takes into account the trade-off between the goodness-of-fit and the complexity of the model. By penalizing the inclusion of unnecessary predictors, adjusted R-squared helps identify the model that explains the most variance in the dependent variable while avoiding overfitting.\n",
    "\n",
    "# 2. Variable Selection: Adjusted R-squared is commonly used in variable selection procedures. When performing feature selection or stepwise regression, adjusted R-squared can guide the selection of predictors. It favors models with a good balance between explanatory power and simplicity. By considering the adjusted R-squared values, one can identify the subset of predictors that contribute the most to explaining the dependent variable.\n",
    "\n",
    "# 3. Model Parsimony: If the goal is to find a model that provides a good fit while using the minimum number of predictors, adjusted R-squared is a suitable metric. It discourages the inclusion of irrelevant predictors and promotes model parsimony. By favoring models with fewer predictors, adjusted R-squared can help create a more interpretable and generalizable model.\n",
    "\n",
    "# 4. Small Sample Size: In cases where the sample size is relatively small, adjusted R-squared is particularly useful. Regular R-squared tends to increase with the addition of more predictors, even if they are not truly meaningful. Adjusted R-squared adjusts for the sample size and degrees of freedom, providing a more reliable measure of the model's performance.\n",
    "\n",
    "# However, it's important to note that adjusted R-squared is not without limitations. It assumes that the model meets the assumptions of linear regression and relies on the correctness of those assumptions. Additionally, adjusted R-squared should not be the sole criterion for model selection or interpretation. Other factors such as substantive knowledge, theoretical considerations, and validation on independent datasets should also be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6c67add-ce5d-45dc-918c-3e10807c17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "400b92e1-fa00-4807-959e-5f2c99e81623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE, MSE, and MAE are commonly used metrics in regression analysis to evaluate the performance of regression models and quantify the prediction accuracy. Here's a description of each metric:\n",
    "\n",
    "# 1. Root Mean Squared Error (RMSE):\n",
    "# RMSE is a measure of the average magnitude of the residuals (prediction errors) in a regression model. It represents the square root of the mean of the squared differences between the predicted values and the actual values. RMSE is calculated as follows:\n",
    "\n",
    "# RMSE = √(MSE) = √(1/n Σ(yᵢ - ŷᵢ)²)\n",
    "\n",
    "# where yᵢ represents the observed actual values, ŷᵢ represents the corresponding predicted values, n is the number of observations, and Σ represents the summation symbol.\n",
    "\n",
    "# RMSE is useful for assessing the overall prediction error and provides an estimate of the standard deviation of the residuals. It is in the same unit as the dependent variable, making it easy to interpret.\n",
    "\n",
    "# 2. Mean Squared Error (MSE):\n",
    "# MSE measures the average squared differences between the predicted values and the actual values. It represents the mean of the squared residuals and is calculated as:\n",
    "\n",
    "# MSE = 1/n Σ(yᵢ - ŷᵢ)²\n",
    "# MSE is obtained by summing up the squared residuals and dividing by the number of observations. It provides a measure of the average prediction error, with larger errors being penalized more due to squaring the differences.\n",
    "\n",
    "# 3. Mean Absolute Error (MAE):\n",
    "# MAE is a measure of the average absolute differences between the predicted values and the actual values. It represents the mean of the absolute residuals and is calculated as:\n",
    "\n",
    "# MAE = 1/n Σ|yᵢ - ŷᵢ|\n",
    "\n",
    "# MAE is obtained by summing up the absolute residuals and dividing by the number of observations. It provides a measure of the average magnitude of the prediction errors without considering their direction.\n",
    "\n",
    "# Interpretation:\n",
    "\n",
    "# RMSE and MSE both measure the prediction error, with lower values indicating better predictive performance. They quantify the average magnitude of the residuals, giving more weight to larger errors in the case of RMSE.\n",
    "# MAE represents the average absolute prediction error, providing a measure of the average magnitude of the errors regardless of their direction.\n",
    "# These metrics are widely used to compare different regression models, select the best model, or assess the accuracy of predictions. The choice of which metric to use depends on the specific context and preferences, but all three provide valuable insights into the quality of the regression model and its predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18eca764-7a20-43d9-9225-3f7d64e7d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a5878d0-46ec-4331-b80f-39a54abb91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages of RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "# 1. Simple Interpretation: RMSE, MSE, and MAE are intuitive metrics that are easy to understand and interpret. They provide a clear measure of the prediction error in the same units as the dependent variable, making it straightforward to compare different models or assess the performance of a single model.\n",
    "\n",
    "# 2. Sensitivity to Outliers: RMSE and MSE, by squaring the differences between predicted and actual values, give more weight to larger errors or outliers. This can be advantageous in scenarios where outliers need to be penalized more heavily, such as in certain anomaly detection or error-sensitive applications.\n",
    "\n",
    "# 3. Differentiating Performance: RMSE, MSE, and MAE enable the comparison of different regression models based on their prediction accuracy. By considering the magnitude of the errors, these metrics can help identify models with superior performance and distinguish them from models with poorer predictive ability.\n",
    "\n",
    "# Disadvantages of RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "# 1. Sensitivity to Scale: RMSE and MSE are sensitive to the scale of the dependent variable, as they involve squaring the differences between predicted and actual values. This means that models with different scales of the dependent variable may produce significantly different RMSE or MSE values. MAE, being based on absolute differences, is less affected by scale but does not capture the variability as effectively.\n",
    "\n",
    "# 2. Lack of Directional Information: RMSE, MSE, and MAE do not provide information about the direction of prediction errors. They treat overestimation and underestimation equally. While this might be acceptable in some cases, in other situations, knowing the direction of errors could be crucial for decision-making or understanding the implications of the model's performance.\n",
    "\n",
    "# 3. Influence of Outliers: While sensitivity to outliers can be an advantage in certain cases, it can also be a disadvantage. RMSE and MSE can be heavily influenced by outliers since they square the differences. This means that a single outlier can disproportionately impact the overall metric, potentially misleading the assessment of model performance.\n",
    "\n",
    "# 4. Lack of Contextual Information: RMSE, MSE, and MAE provide a summary measure of prediction accuracy but do not reveal the underlying patterns or specific errors. They may not capture the nuances of specific cases or account for variations in prediction errors across different regions of the data space.\n",
    "\n",
    "# It is important to consider these advantages and disadvantages while selecting the appropriate evaluation metric(s) for regression analysis. The choice should be guided by the specific objectives of the analysis, the nature of the data, and the context in which the model will be used. In some cases, it may be useful to consider a combination of metrics to gain a more comprehensive understanding of the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0776b7c2-1091-4c79-921b-2920ff73ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1dc6e679-7f23-481f-84af-ffe2b7ff08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regularization, also known as L1 regularization, is a technique used in regression analysis to introduce a penalty term that encourages sparsity in the regression coefficients. It adds a regularization term to the loss function of the linear regression model, which is the sum of the absolute values of the coefficients multiplied by a regularization parameter (lambda or alpha).\n",
    "\n",
    "# The Lasso regularization technique differs from Ridge regularization (L2 regularization) in the type of penalty applied to the coefficients. While Ridge regularization penalizes the sum of the squared coefficients, Lasso regularization penalizes the sum of the absolute values of the coefficients.\n",
    "\n",
    "# The key characteristics of Lasso regularization are as follows:\n",
    "\n",
    "# 1. Sparsity: Lasso regularization tends to drive some regression coefficients to exactly zero. This property allows it to perform feature selection by effectively excluding less relevant variables from the model. As a result, Lasso can produce models with a smaller set of predictors compared to Ridge regularization.\n",
    "\n",
    "# 2. Automatic Feature Selection: Lasso regularization can automatically select important features and eliminate irrelevant or redundant ones. It encourages the coefficients of less important variables to become zero, effectively removing them from the model. This can simplify the model interpretation and improve its generalization capability.\n",
    "\n",
    "# 3. Non-Constant Shrinkage: Lasso regularization shrinks the coefficients by a constant factor but can completely eliminate some coefficients. It does not provide a gradual shrinkage of all coefficients as in Ridge regularization.\n",
    "\n",
    "# 4. Increased Bias: Lasso regularization introduces more bias in the estimated coefficients compared to Ridge regularization. This bias helps prevent overfitting but can lead to a higher level of underfitting if the true underlying model contains important variables with small coefficients.\n",
    "\n",
    "# When to use Lasso regularization:\n",
    "\n",
    "# 1. Feature Selection: When there is a large number of predictors, and it is desirable to identify a subset of relevant variables, Lasso regularization is more appropriate. It can effectively shrink some coefficients to zero, indicating the exclusion of corresponding predictors from the model.\n",
    "\n",
    "# 2. Sparse Solutions: When dealing with high-dimensional data where most predictors are irrelevant or have small effects, Lasso regularization can produce sparse solutions, simplifying the model and improving interpretability.\n",
    "\n",
    "# 3. Trade-off between Bias and Variance: Lasso regularization can be preferred when the trade-off between model bias and variance needs to be carefully managed. It can strike a balance by introducing bias through coefficient shrinkage and reducing variance by selecting fewer predictors.\n",
    "\n",
    "# It's worth noting that the choice between Lasso regularization and Ridge regularization depends on the specific problem, the nature of the data, and the goals of the analysis. Cross-validation and other model selection techniques can be employed to determine which regularization approach performs better for a given dataset and task.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "955ca127-5b67-42a6-9718-93a26d9ea024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de4a7091-e8f2-4171-a7f1-97231f58bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized linear models help prevent overfitting in machine learning by introducing a penalty term that discourages large coefficients or complex models. This penalty term encourages the model to find a balance between fitting the training data well and keeping the model's complexity in check.\n",
    "\n",
    "# One common form of regularization is the addition of a regularization term to the loss function of a linear regression model. This regularization term is typically a function of the regression coefficients and is multiplied by a regularization parameter (lambda or alpha). There are two commonly used regularization techniques: Ridge regularization (L2 regularization) and Lasso regularization (L1 regularization).\n",
    "\n",
    "#Let's take an example to illustrate how regularized linear models prevent overfitting. Consider a scenario where we have a dataset with 100 observations and 1000 predictors. We want to build a linear regression model to predict a target variable. Without regularization, the model could potentially fit the noise in the training data, leading to overfitting. Regularized linear models can help address this issue.\n",
    "\n",
    "# 1. Ridge Regularization (L2 regularization):\n",
    "# Ridge regression adds a penalty term to the loss function, which is the sum of squared coefficients multiplied by a regularization parameter (lambda). The higher the value of lambda, the greater the penalty on large coefficients.\n",
    "\n",
    "# By applying Ridge regularization, the model will shrink the coefficients towards zero, but not exactly to zero. This helps to reduce the impact of irrelevant or noise variables while still keeping them in the model to some extent.\n",
    "\n",
    "# 2. Lasso Regularization (L1 regularization):\n",
    "# Lasso regression also adds a penalty term to the loss function, which is the sum of the absolute values of the coefficients multiplied by a regularization parameter (lambda). Lasso regularization has the property of inducing sparsity, i.e., driving some coefficients to exactly zero.\n",
    "\n",
    "# With Lasso regularization, the model can effectively perform feature selection by eliminating irrelevant predictors. It selects the most important predictors while setting the coefficients of less important predictors to zero, thus simplifying the model.\n",
    "\n",
    "# In both cases, Ridge and Lasso regularization prevent overfitting by reducing the model's complexity. By adding a penalty term to the loss function, the models are discouraged from fitting the noise in the training data, leading to more robust and generalized predictions.\n",
    "\n",
    "# In summary, regularized linear models help prevent overfitting by introducing a penalty term that controls the model's complexity. They strike a balance between fitting the training data well and avoiding excessive reliance on irrelevant predictors. The choice between Ridge and Lasso regularization depends on the specific problem and the desired characteristics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d00d7cb-b058-427f-a159-9ce34d3ceaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b57eac88-9fe1-420f-a603-b23be00b232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While regularized linear models have several advantages, they also have limitations that make them not always the best choice for regression analysis. Here are some limitations to consider:\n",
    "\n",
    "# 1. Linear Assumption: Regularized linear models assume a linear relationship between predictors and the target variable. If the underlying relationship is highly non-linear or exhibits complex interactions, regularized linear models may not capture the full complexity of the data. In such cases, more flexible models like nonlinear regression or tree-based models might be more appropriate.\n",
    "\n",
    "# 2. Feature Scaling: Regularized linear models are sensitive to the scale of the predictors. If the predictors have different scales, the regularization may not be applied uniformly, leading to biased coefficient estimates. It is important to scale the predictors before applying regularization to ensure fair treatment of all variables.\n",
    "\n",
    "# 3. Over-regularization: Regularization can be overly restrictive, leading to underfitting. If the regularization parameter (lambda or alpha) is set too high, the model may excessively shrink the coefficients, resulting in a simplified model that fails to capture important relationships in the data. Finding the right balance between regularization and model complexity is crucial.\n",
    "\n",
    "# 4. Feature Interpretability: Regularized linear models may shrink some coefficients to zero, effectively excluding the corresponding predictors from the model. While this can help with feature selection, it may sacrifice the interpretability of the model. If interpretability is a priority, alternative methods that provide clearer explanations of the relationships between predictors and the target variable might be preferred.\n",
    "\n",
    "# 5. Non-Unique Solutions: Regularized linear models can yield non-unique solutions when predictors are highly correlated. In such cases, the choice of which predictor(s) to keep or exclude can be arbitrary, leading to different model outcomes. Careful consideration is required to interpret and select the relevant predictors appropriately.\n",
    "\n",
    "# 6.Outliers and Robustness: Regularized linear models can be sensitive to outliers, especially in L2 regularization. Outliers can disproportionately influence the regularization term and, in turn, the coefficient estimates. Robust regression methods might be more suitable when dealing with datasets containing influential outliers.\n",
    "\n",
    "# 7. Computational Complexity: For large-scale datasets with a high number of predictors, regularized linear models can be computationally expensive to train and evaluate. As the number of predictors increases, the computational cost of solving the optimization problem grows. Alternative techniques like dimensionality reduction or specialized algorithms may be more efficient in such cases.\n",
    "\n",
    "# In summary, while regularized linear models have many benefits, they are not always the best choice for regression analysis. The appropriateness of regularized linear models depends on the specific characteristics of the data, the nature of the relationships between predictors and the target variable, and the desired interpretability of the model. It is essential to carefully evaluate the limitations and assumptions of regularized linear models before deciding on their usage in regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7570fd46-aa13-4e7a-b6cf-5007a42a1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d76c5647-e027-4916-b607-4b8e1b4bb752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the given scenario, we have Model A with an RMSE of 10 and Model B with an MAE of 8. To determine which model is the better performer, we need to consider the evaluation metrics and their characteristics.\n",
    "\n",
    "# RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) are both commonly used evaluation metrics in regression analysis, but they capture different aspects of the prediction errors:\n",
    "\n",
    "# 1. RMSE: RMSE measures the average magnitude of the squared differences between predicted and actual values. It is more sensitive to large errors or outliers in the data. RMSE is also in the same unit as the dependent variable, which can be beneficial for interpretation.\n",
    "\n",
    "# 2. MAE: MAE measures the average magnitude of the absolute differences between predicted and actual values. It treats all errors equally, without emphasizing the impact of outliers.\n",
    "\n",
    "# Based on the provided metrics, Model B has a lower MAE (8) compared to Model A's RMSE (10). In this case, we would choose Model B as the better performer. The lower MAE indicates that, on average, Model B's predictions deviate from the actual values by a smaller amount compared to Model A.\n",
    "\n",
    "# However, it is important to acknowledge the limitations of these metrics:\n",
    "\n",
    "# 1. Sensitivity to Scale: RMSE and MAE are sensitive to the scale of the dependent variable. If the dependent variable has a large scale or outliers, the magnitude of the metrics can be influenced, making direct comparisons across different datasets or variables challenging.\n",
    "\n",
    "# 2. Emphasis on Errors: Both RMSE and MAE focus solely on the errors between predicted and actual values. They do not provide information about the direction or patterns of the errors, which can be relevant in certain contexts.\n",
    "\n",
    "# 3. Contextual Considerations: The choice of the evaluation metric should be aligned with the specific objectives and requirements of the problem. For example, in some cases, minimizing the impact of large errors (RMSE) might be more important than focusing on the average magnitude of errors (MAE), or vice versa.\n",
    "\n",
    "# Therefore, while Model B seems to be the better performer based on the provided metrics, it is crucial to consider the limitations and context of the problem when interpreting and comparing evaluation metrics. Additionally, it is recommended to use multiple evaluation metrics and perform further analysis to gain a comprehensive understanding of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d247e6c-7e82-45c5-8561-f21d3869fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e7a7c-bfa8-42b0-87e7-ac5bae16ce78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
